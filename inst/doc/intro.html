<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">



<title>Introduction to glmnetUtils</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Introduction to glmnetUtils</h1>



<p>The <a href="https://github.com/hong-revo/glmnetUtils">glmnetUtils package</a> provides a collection of tools to streamline the process of fitting elastic net models with <a href="https://cran.r-project.org/package=glmnet">glmnet</a>. I wrote the package after a couple of projects where I found myself writing the same boilerplate code to convert a data frame into a predictor matrix and a response vector. In addition to providing a formula interface, it also features a function <code>cva.glmnet</code> to do crossvalidation for both <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span>, as well as some utility functions.</p>
<div id="the-formula-interface" class="section level2">
<h2>The formula interface</h2>
<p>The interface that glmnetUtils provides is very much the same as for most modelling functions in R. To fit a model, you provide a formula and data frame. You can also provide any arguments that glmnet will accept. Here are some simple examples for different types of data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># least squares regression</span>
(mtcarsMod &lt;-<span class="st"> </span><span class="kw">glmnet</span>(mpg ~<span class="st"> </span>cyl +<span class="st"> </span>disp +<span class="st"> </span>hp, <span class="dt">data=</span>mtcars))</code></pre></div>
<pre><code>## Call:
## glmnet.formula(formula = mpg ~ cyl + disp + hp, data = mtcars)
## 
## Model fitting options:
##     Sparse model matrix: FALSE
##     Use model.frame: FALSE
##     Alpha: 1
##     Lambda summary:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## 0.03326 0.11690 0.41000 1.02800 1.44100 5.05500</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># multinomial logistic regression with specified elastic net alpha parameter</span>
(irisMod &lt;-<span class="st"> </span><span class="kw">glmnet</span>(Species ~<span class="st"> </span>., <span class="dt">data=</span>iris, <span class="dt">family=</span><span class="st">&quot;multinomial&quot;</span>, <span class="dt">alpha=</span><span class="fl">0.5</span>))</code></pre></div>
<pre><code>## Call:
## glmnet.formula(formula = Species ~ ., data = iris, alpha = 0.5, 
##     family = &quot;multinomial&quot;)
## 
## Model fitting options:
##     Sparse model matrix: FALSE
##     Use model.frame: FALSE
##     Alpha: 0.5
##     Lambda summary:
##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
## 0.0000870 0.0008707 0.0087090 0.0979200 0.0870700 0.8700000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Poisson regression with an offset</span>
(InsMod &lt;-<span class="st"> </span><span class="kw">glmnet</span>(Claims ~<span class="st"> </span>District +<span class="st"> </span>Group +<span class="st"> </span>Age, <span class="dt">data=</span>MASS::Insurance,
                  <span class="dt">family=</span><span class="st">&quot;poisson&quot;</span>, <span class="dt">offset=</span><span class="kw">log</span>(Holders)))</code></pre></div>
<pre><code>## Call:
## glmnet.formula(formula = Claims ~ District + Group + Age, data = MASS::Insurance, 
##     family = &quot;poisson&quot;, offset = log(Holders))
## 
## Model fitting options:
##     Sparse model matrix: FALSE
##     Use model.frame: FALSE
##     Alpha: 1
##     Lambda summary:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## 0.02877 0.11610 0.46880 1.40500 1.89300 7.64100</code></pre>
<p>Under the hood, glmnetUtils creates a model matrix and response vector, and passes them to the glmnet package to do the actual model fitting. A simple <code>print</code> method is also provided, to show the main model details at a glance. I’ll describe shortly what the “sparse model matrix” and “use model.frame” options do.</p>
<p>Predicting from a model works as you’d expect: just pass a data frame containing the new observations to the <code>predict</code> method. You can also specify any arguments that <code>predict.glmnet</code> accepts.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># least squares regression: get predictions for lambda=1</span>
<span class="kw">predict</span>(mtcarsMod, <span class="dt">newdata=</span>mtcars, <span class="dt">s=</span><span class="dv">1</span>)

<span class="co"># multinomial logistic regression: get predicted class</span>
<span class="kw">predict</span>(irisMod, <span class="dt">newdata=</span>iris, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)

<span class="co"># Poisson regression: need to specify offset</span>
<span class="kw">predict</span>(InsMod, <span class="dt">newdata=</span>MASS::Insurance, <span class="dt">offset=</span><span class="kw">log</span>(Holders))</code></pre></div>
<p>If you want, you can still use the original model matrix-plus-response syntax:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcarsX &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(mtcars[<span class="kw">c</span>(<span class="st">&quot;cyl&quot;</span>, <span class="st">&quot;disp&quot;</span>, <span class="st">&quot;hp&quot;</span>)])
mtcarsY &lt;-<span class="st"> </span>mtcars$mpg
mtcarsMod2 &lt;-<span class="st"> </span><span class="kw">glmnet</span>(mtcarsX, mtcarsY)

<span class="kw">summary</span>(<span class="kw">as.numeric</span>(<span class="kw">predict</span>(mtcarsMod, mtcars) -<span class="st"> </span>
<span class="st">                   </span><span class="kw">predict</span>(mtcarsMod2, mtcarsX)))</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##       0       0       0       0       0       0</code></pre>
<p>This shows that the resulting models are identical, in terms of the predictions they make and the regularisation parameters used.</p>
</div>
<div id="generating-the-model-matrix" class="section level2">
<h2>Generating the model matrix</h2>
<p>There are two ways in which glmnetUtils can generate a model matrix out of a formula and data frame. The first is to use the standard R machinery comprising <code>model.frame</code> and <code>model.matrix</code>; and the second is to build the matrix one variable at a time.</p>
<div id="using-model.frame" class="section level3">
<h3>Using <code>model.frame</code></h3>
<p>This is the simpler option, and the one that is most compatible with other R modelling functions. The <code>model.frame</code> function takes a formula and data frame and returns a <em>model frame</em>: a data frame with special information attached that lets R make sense of the terms in the formula. For example, if a formula includes an interaction term, the model frame will specify which columns in the data relate to the interaction, and how they should be treated. Similarly, if the formula includes expressions like <code>exp(x)</code> or <code>I(x^2)</code> on the RHS, <code>model.frame</code> will evaluate these expressions and include them in the output.</p>
<p>The major disadvantage of using <code>model.frame</code> is that it generates a <code>terms</code> object, which encodes how variables and interactions are organised. One of the attributes of this object is a matrix with one row per variable, and one column per main effect and interaction. At minimum, this is (approximately) a <span class="math inline">\(p \times p\)</span> square matrix where <span class="math inline">\(p\)</span> is the number of main effects in the model. For wide datasets with <span class="math inline">\(p &gt; 10000\)</span>, this matrix can approach or exceed a gigabyte in size. Even if there is enough memory to store such an object, generating the model matrix can be very slow.</p>
<p>Another issue with the standard R approach is the treatment of factors. Normally, model.matrix will turn an <span class="math inline">\(N\)</span>-level factor into an indicator matrix with <span class="math inline">\(N-1\)</span> columns, with one column being dropped. This is necessary for unregularised models as fit with <code>lm</code> and <code>glm</code>, since the full set of <span class="math inline">\(N\)</span> columns is linearly dependent. With the usual <a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/contrasts.html">treatment contrasts</a>, the interpretation is that the dropped column represents a baseline level, while the coefficients for the other columns represent the difference in the response relative to the baseline.</p>
<p>This may not be appropriate for a regularised model as fit with glmnet. The regularisation procedure shrinks the coefficients towards zero, which forces the estimated differences from the baseline to be smaller. But this only makes sense if the baseline level was chosen beforehand, or is otherwise meaningful as a default; otherwise it is effectively making the levels more similar to an arbitrarily chosen level.</p>
</div>
<div id="manually-building-the-model-matrix" class="section level3">
<h3>Manually building the model matrix</h3>
<p>To deal with the problems above, glmnetUtils by default will avoid using <code>model.frame</code>, instead building up the model matrix term-by-term. This avoids the memory cost of creating a <code>terms</code> object, and can be noticeably faster than the standard approach. It will also include one column in the model matrix for <em>all</em> levels in a factor; that is, no baseline level is assumed. In this situation, the coefficients represent differences from the overall mean response, and shrinking them to zero <em>is</em> meaningful (usually).</p>
<p>This works in an additive fashion, ie the formula <code>~ a + b:c + d*e</code> is treated as consisting of three terms, <code>a</code>, <code>b:c</code> and <code>d*e</code> each of which is processed independently of the others. A dot in the formula includes all main effect terms, ie <code>~ . + a:b + f(x)</code> expands to <code>~ a + b + x + a:b + f(x)</code> (assuming a, b and x are the only columns in the data). Note that a formula like <code>~ (a + b) + (c + d)</code> will be treated as two terms, <code>a + b</code> and <code>c + d</code>.</p>
</div>
<div id="speed-comparisons" class="section level3">
<h3>Speed comparisons</h3>
<p>To examine the speed impact of using <code>model.frame</code>, let’s do some simple comparisons of run times. We’ll generate sample datasets with 100, 1,000 and 10,000 predictors, and then run <code>glmnet</code> with both options for generating the model matrix.</p>
<!--- don't eval: speed up vignette build -->
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate sample (uncorrelated) data of a given size</span>
makeSampleData &lt;-<span class="st"> </span>function(N, P)
{
    X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(N*P), <span class="dt">nrow=</span>N)
    <span class="kw">data.frame</span>(<span class="dt">y=</span><span class="kw">rnorm</span>(N), X)
}

<span class="co"># test for three sizes: 100/1000/10000 predictors</span>
df1 &lt;-<span class="st"> </span><span class="kw">makeSampleData</span>(<span class="dt">N=</span><span class="dv">1000</span>, <span class="dt">P=</span><span class="dv">100</span>)
df2 &lt;-<span class="st"> </span><span class="kw">makeSampleData</span>(<span class="dt">N=</span><span class="dv">1000</span>, <span class="dt">P=</span><span class="dv">1000</span>)
df3 &lt;-<span class="st"> </span><span class="kw">makeSampleData</span>(<span class="dt">N=</span><span class="dv">1000</span>, <span class="dt">P=</span><span class="dv">10000</span>)

<span class="kw">library</span>(microbenchmark)
res &lt;-<span class="st"> </span><span class="kw">microbenchmark</span>(
    <span class="kw">glmnet</span>(y ~<span class="st"> </span>., df1, <span class="dt">use.model.frame=</span><span class="ot">TRUE</span>), 
    <span class="kw">glmnet</span>(y ~<span class="st"> </span>., df1, <span class="dt">use.model.frame=</span><span class="ot">FALSE</span>), 
    <span class="kw">glmnet</span>(y ~<span class="st"> </span>., df2, <span class="dt">use.model.frame=</span><span class="ot">TRUE</span>), 
    <span class="kw">glmnet</span>(y ~<span class="st"> </span>., df2, <span class="dt">use.model.frame=</span><span class="ot">FALSE</span>), 
    <span class="kw">glmnet</span>(y ~<span class="st"> </span>., df3, <span class="dt">use.model.frame=</span><span class="ot">TRUE</span>), 
    <span class="kw">glmnet</span>(y ~<span class="st"> </span>., df3, <span class="dt">use.model.frame=</span><span class="ot">FALSE</span>),
    <span class="dt">times=</span><span class="dv">10</span> 
)
<span class="kw">print</span>(res, <span class="dt">unit=</span><span class="st">&quot;s&quot;</span>, <span class="dt">digits=</span><span class="dv">2</span>)</code></pre></div>
<!--- previous output -->
<pre><code>## Unit: seconds
##                                         expr    min     lq   mean median     uq    max neval
##   glmnet(y ~ ., df1, use.model.frame = TRUE)  0.024  0.024  0.027  0.025  0.029  0.032    10
##  glmnet(y ~ ., df1, use.model.frame = FALSE)  0.023  0.026  0.029  0.026  0.028  0.051    10
##   glmnet(y ~ ., df2, use.model.frame = TRUE)  3.703  3.916  4.258  4.272  4.428  5.153    10
##  glmnet(y ~ ., df2, use.model.frame = FALSE)  3.756  3.874  4.291  4.352  4.561  5.073    10
##   glmnet(y ~ ., df3, use.model.frame = TRUE) 11.973 12.353 13.262 13.350 13.864 14.622    10
##  glmnet(y ~ ., df3, use.model.frame = FALSE)  4.295  4.639  4.992  4.822  5.060  6.111    10</code></pre>
<p>From this, we can see that for datasets with up to 1,000 predictors, both methods are about as fast as each other. However, for 10,000 predictors (not uncommon these days), the <code>model.frame</code> method takes three times as long as building the model matrix term by term.</p>
<p>What happens if we take it up to 100,000 predictors? As seen below, the standard approach of using <code>model.frame</code> fails when R runs out of memory: the data frame itself is about 800MB in size, but trying to allocate the <code>terms</code> object requires more then 67GB. However, building the model matrix by term still works, and (on this machine) finishes in about two minutes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df4 &lt;-<span class="st"> </span><span class="kw">makeSampleData</span>(<span class="dt">N=</span><span class="dv">1000</span>, <span class="dt">P=</span><span class="dv">100000</span>)

<span class="kw">glmnet</span>(y ~<span class="st"> </span>., df4, <span class="dt">use.model.frame=</span><span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## Warning in terms.formula(formula, data = data): Reached total allocation of
## 32666Mb: see help(memory.size)

## Error: cannot allocate vector of size 37.3 Gb</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glmnet</span>(y ~<span class="st"> </span>., df4, <span class="dt">use.model.frame=</span><span class="ot">FALSE</span>)</code></pre></div>
<pre><code>## Call:
## glmnet.formula(formula = y ~ ., data = df4, use.model.frame = FALSE)
## 
## Model fitting options:
##     Sparse model matrix: FALSE
##     Use model.frame: FALSE
##     Alpha: 1
##     Lambda summary:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## 0.002393 0.006506 0.017680 0.032470 0.048080 0.130700</code></pre>
</div>
<div id="sparse-model-matrix" class="section level3">
<h3>Sparse model matrix</h3>
<p>As an option, glmnetUtils can also generate a <em>sparse</em> model matrix, using the <code>sparse.model.matrix</code> function provided in the Matrix package. This works exactly the same as a regular model matrix, but takes up significantly less memory if many of its entries are zero. A scenario where this is the case would be where many of the predictors are factors, each with a large number of levels. This can be combined with both of the previously mentioned options for generating model matrices.</p>
</div>
</div>
<div id="crossvalidation-for-alpha" class="section level2">
<h2>Crossvalidation for <span class="math inline">\(\alpha\)</span></h2>
<p>One piece missing from the standard glmnet package is a way of choosing <span class="math inline">\(\alpha\)</span>, the elastic net mixing parameter, similar to how <code>cv.glmnet</code> chooses <span class="math inline">\(\lambda\)</span>, the shrinkage parameter. To fix this, glmnetUtils provides the <code>cva.glmnet</code> function, which uses crossvalidation to examine the impact on the model of changing <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span>. The interface is the same as for the other functions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Leukemia dataset from Trevor Hastie's website:</span>
<span class="co"># http://web.stanford.edu/~hastie/glmnet/glmnetData/Leukemia.RData</span>
leuk &lt;-<span class="st"> </span><span class="kw">do.call</span>(data.frame, Leukemia)

leukMod &lt;-<span class="st"> </span><span class="kw">cva.glmnet</span>(y ~<span class="st"> </span>., <span class="dt">data=</span>leuk, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)
leukMod</code></pre></div>
<pre><code>## Call:
## cva.glmnet.formula(formula = y ~ ., data = leuk, family = &quot;binomial&quot;)
## 
## Model fitting options:
##     Sparse model matrix: FALSE
##     Use model.frame: FALSE
##     Alpha values: 0 0.001 0.008 0.027 0.064 0.125 0.216 0.343 0.512 0.729 1
##     Number of crossvalidation folds for lambda: 10</code></pre>
<p><code>cva.glmnet</code> uses the algorithm described in the help for <code>cv.glmnet</code>, which is to fix the distribution of observations across folds and then call <code>cv.glmnet</code> in a loop with different values of <span class="math inline">\(\alpha\)</span>. Optionally, you can parallelise this outer loop, by setting the <code>outerParallel</code> argument to a non-NULL value. Currently, glmnetUtils supports the following methods of parallelisation:</p>
<ul>
<li>Via <code>parLapply</code> in the parallel package. To use this, set <code>outerParallel</code> to a valid cluster object created by <code>makeCluster</code>.</li>
<li>Via <code>rxExec</code> as supplied by <a href="https://www.microsoft.com/en-au/cloud-platform/r-server">Microsoft R Server’s</a> RevoScaleR package. To use this, set <code>outerParallel</code> to a valid compute context created by <code>RxComputeContext</code>, or a character string specifying such a context.</li>
</ul>
<p>If the outer loop is run in parallel, <code>cva.glmnet</code> can check if the inner loop (over <span class="math inline">\(\lambda\)</span>) is also set to run in parallel, and disable this if it would lead to contention for cores.</p>
<p>Because crossvalidation is often a statistically noisy procedure, it doesn’t try to automatically choose <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span> for you. Instead you can plot the output, to see how the results depend on the values of these parameters. Using this information, you can choose appropriate values for your data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(leukMod)</code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>In this case, we see that values of <span class="math inline">\(\alpha\)</span> close to <span class="math inline">\(1\)</span> tend to lead to better accuracy. The curves don’t have a well-defined minimum, but they do flatten out for lower values of <span class="math inline">\(\lambda\)</span>. As the <code>cv.glmnet</code> documentation recommends though, it’s a good idea to run <code>cva.glmnet</code> multiple times to reduce the impact of noise.</p>
<p>A <code>cva.glmnet</code> object contains a list of individual <code>cv.glmnet</code> objects, corresponding to the different <span class="math inline">\(\alpha\)</span> values tried. This lets you plot the crossvalidation results easily for a given <span class="math inline">\(\alpha\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(leukMod$modlist[[<span class="dv">10</span>]])  <span class="co"># alpha = 0.729</span></code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>The glmnetUtils package is a way to improve quality of life for users of glmnet. As with many R packages, it’s always under development; you can get the latest version from my <a href="https://github.com/Hong-Revo/glmnetUtils">GitHub repo</a>. If you find a bug, or if you want to suggest improvements to the package, please feel free to contact me at <a href="mailto:hongooi@microsoft.com">hongooi@microsoft.com</a>.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
